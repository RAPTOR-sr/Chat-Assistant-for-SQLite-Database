{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "import bitsandbytes\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig,get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "MODEL_NAME = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly setting the tokenizer type\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True,  # Allow loading custom tokenizer implementations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with 8-bit quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True  # Enable 8-bit quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=16,                # rank of LoRA updates\n",
    "    lora_alpha=32,       # scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # adjust target modules as needed\n",
    "    lora_dropout=0.1,    # dropout for LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model with PEFT/LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(\"LoRA model parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'D:/Projects/Chat Assistant for SQLite Database/FineTune/SQL_CustumDataSet/spider_text_sql.csv'\n",
    "MAX_SEQ_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    # Concatenate the 'intput' and 'output' columns with a newline and EOS token.\n",
    "    text = example[\"text_query\"].strip() + \"\\n\" + example[\"sql_command\"].strip() + tokenizer.eos_token\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Load the CSV dataset.\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": DATA_FILE})\n",
    "# Map each example to a unified text field.\n",
    "dataset = dataset[\"train\"].map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset.\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data collator for language modeling.\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek_r1_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # simulate a larger batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-2,\n",
    "    logging_steps=5,\n",
    "    save_steps=50,\n",
    "    fp16=True,  # mixed precision if supported\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Colab\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./deepseek_r1_finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Simulates a larger batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,  # Reduced learning rate for stability\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    save_steps=100,  # Save checkpoint every 100 steps\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model and Tokenizer\n",
    "output_dir = 'fine_tuned_deepseek'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CudaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
